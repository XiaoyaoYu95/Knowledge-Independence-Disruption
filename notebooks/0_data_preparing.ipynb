{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3391703",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "This section provides the code that extracts data from the SciSciNet dataset, which is necessary. Readers need to download the original SciSciNet data from https://doi.org/10.6084/m9.figshare.c.6076908.v1 to the path \"data/raw/\". The files that we will use in this study contains: SciSciNet_Papers.tsv, SciSciNet_PaperReferences.tsv, SciSciNet_Fields.tsv, SciSciNet_PaperFields.tsv, SciSciNet_PaperAuthorAffiliations.tsv, SciSciNet_Affiliations.tsv and SciSciNet_Journals.tsv. For journal rank, we apply the Scimago Journal Rank (SJR), which is avaliable at https://www.scimagojr.com/journalrank.php.\n",
    "\n",
    "\n",
    "Considering the scale of raw data and the time-consuming process, we provide the samples of source code for further check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42755469",
   "metadata": {},
   "source": [
    "## Efficient citation matrix\n",
    "Since our study centers on citation relationships among scientific papers, constructing an efficient citation matrix is essential. We focus on two aspects of citation behavior: (1) how a paper cites other papers (i.e., its reference list), and (2) how a paper is cited by subsequent papers (i.e., its received citations).\n",
    "\n",
    "To meet these needs, we utilize the scipy.sparse.csr_matrix format to represent the full citation network. The csr_matrix is well-suited for efficient row slicing, which facilitates quick access to a paper's references. When efficient column slicing is required—for retrieving a paper's received citations—we convert the matrix to csc_matrix, which supports fast column operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29dce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "pre_path = os.path.abspath(r\"..\")\n",
    "sys.path.insert(1, os.path.join(pre_path, 'src'))\n",
    "from utils import read_big_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1c263",
   "metadata": {},
   "source": [
    "#### Map the original PaperID to a new PaperID in scipy.sparse.csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Citing_PaperID-Cited_PaperID citation pairs. \n",
    "Paper_Reference_df = read_big_csv(\"%s/data/raw/SciSciNet_PaperReferences.tsv\"%pre_path, sep='\\t', compression=None, chunksize=1000000, nrows=None, \n",
    "                           usecols=['Citing_PaperID', 'Cited_PaperID'])\n",
    "citing_list_raw, cited_list_raw = Paper_Reference_df['Citing_PaperID'].tolist(), Paper_Reference_df['Cited_PaperID'].tolist()\n",
    "del Paper_Reference_df\n",
    "gc.collect()\n",
    "\n",
    "Paper_newID = {} # Paper_newID is a dictionary that maps the original paperID to a new ID from 0 to max_num.\n",
    "max_num = 0 # the maximum number of paperID in the new ID mapping.\n",
    "for p_citing in citing_list_raw:\n",
    "    if p_citing not in Paper_newID:\n",
    "        Paper_newID[p_citing] = max_num\n",
    "        max_num += 1\n",
    "for p_cited in cited_list_raw:\n",
    "    if p_cited not in Paper_newID:\n",
    "        Paper_newID[p_cited] = max_num\n",
    "        max_num += 1\n",
    "# Save the new ID mapping to a pickle file for further alignment.\n",
    "pickle.dump(Paper_newID, open(\"%s/data/processed/Paper_newID.pickle\"%pre_path, 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42311b3d",
   "metadata": {},
   "source": [
    "#### Construct a scipy.sparse.csr_matrix by the new PaperID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute the original paperID with the new ID.\n",
    "citing_list_new = [Paper_newID[p] for p in citing_list_raw]\n",
    "# return the indices that would sort the array, to ensure the same citing papers cluster together.\n",
    "idx_list = np.argsort(citing_list_new)\n",
    "# sort the cited_list_new according to the sorted citing_list_new, to keep the citing-cited pairs.\n",
    "cited_list_new = [Paper_newID[cited_list_raw[i]] for i in idx_list]\n",
    "del citing_list_raw,cited_list_raw,Paper_newID\n",
    "gc.collect()\n",
    "\n",
    "citing_id_counter = Counter(citing_list_new) # Count the number of references for each citing paper. Paper not in the citing list will be assigned a default value zero.\n",
    "del citing_list_new\n",
    "gc.collect()\n",
    "\n",
    "indices = [] # indices will store the indices of the cited papers in the citation matrix.\n",
    "indptr = [0]*(max_num+1) # indptr will store the starting index of each citing paper's references in the indices list. \n",
    "for ix in range(max_num):\n",
    "    start_idx = indptr[ix]\n",
    "    end_idx = start_idx + citing_id_counter[ix]\n",
    "    if end_idx != start_idx: # if the citing paper has references.\n",
    "        # Get the indices of the set of cited papers for the current citing paper.\n",
    "        ref_list_ix = list(set(cited_list_new[start_idx:end_idx]))\n",
    "        if ref_list_ix: # if the citing paper has references.\n",
    "            indices.extend(ref_list_ix)\n",
    "            indptr[ix+1] = start_idx + len(ref_list_ix) # Update the starting index for the next citing paper.\n",
    "        else:\n",
    "            indptr[ix+1] = start_idx\n",
    "    else:\n",
    "        indptr[ix+1] = start_idx\n",
    "del citing_id_counter,cited_list_new\n",
    "gc.collect()\n",
    "\n",
    "# Create a sparse matrix in CSR format to represent the citation network.\n",
    "data = np.ones(len(indices),dtype = bool)\n",
    "sparse_csr_matrix = scipy.sparse.csr_matrix((data, np.array(indices), np.array(indptr)), shape=(max_num, max_num))\n",
    "del data,indices,indptr\n",
    "gc.collect()\n",
    "pickle.dump(sparse_csr_matrix, open(\"%s/data/processed/citation_matrix_csr.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca44a63",
   "metadata": {},
   "source": [
    "## Paper properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2099a65",
   "metadata": {},
   "source": [
    "#### Paper DocType, publication year, publication date, reference novelty (10pct), sleeping beauty index, and grant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extract the properties of papers recorded in the SciSciNet dataset \"SciSciNet_Papers.tsv\", including:\n",
    "paper's DocType, publication year, publication date, reference novelty (10pct), sleeping beauty index, and grant information. \n",
    "Each pair of relationship \"PaperNewID-property\" is kept in a dictionary.\n",
    "'''\n",
    "\n",
    "# Load the Paper_newID mapping from the pickle file.\n",
    "Paper_newID = pickle.load(open(\"%s/data/processed/Paper_newID.pickle\"%pre_path, 'rb'))\n",
    "Paper_newID_df = pd.DataFrame(list(Paper_newID.items()), columns=['PaperID', 'PaperNewID'])\n",
    "\n",
    "\n",
    "# Load the SciSciNet_Papers.tsv file and extract the properties of interest.\n",
    "########### SciSciNet_Papers.tsv\n",
    "property_list1 = ['PaperID','DocType','Year','Date','Atyp_10pct_Z','SB_B','SB_T','NIH_Count','NSF_Count']\n",
    "Papers_df = read_big_csv(\"%s/data/raw/SciSciNet_Papers.tsv\"%pre_path, sep='\\t', compression=None, chunksize=1000000, nrows=None, \n",
    "                           usecols=property_list1)\n",
    "Papers_df = Papers_df.merge(Paper_newID_df, on='PaperID', how='left')  # Merge the new ID mapping with the properties DataFrame.\n",
    "\n",
    "for property in property_list1[1:]:  # Skip 'PaperID' as it is not a property.\n",
    "    DF_temp = Papers_df[['PaperNewID', property]]\n",
    "    DF_temp = DF_temp.dropna(subset=['PaperNewID', property]) # Drop rows with NaN values in either column.\n",
    "\n",
    "    Paper_dict = DF_temp.set_index(\"PaperNewID\")[property].to_dict()\n",
    "    pickle.dump(Paper_dict, open(\"%s/data/processed/PaperID_%s.pickle\"%(pre_path,property), 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "# Load the SciSciNet_PaperFields.tsv and SciSciNet_Fields.tsv file to extract the top field of papers.\n",
    "########### SciSciNet_PaperFields.tsv\n",
    "Paper_Fields_df = read_big_csv(\"%s/data/raw/SciSciNet_PaperFields.tsv\"%pre_path, sep='\\t', compression=None, chunksize=1000000, nrows=None, \n",
    "                           usecols=['PaperID','FieldID'])\n",
    "Paper_Fields_df = Paper_Fields_df.merge(Paper_newID_df, on='PaperID', how='left')  # Merge the new ID mapping with the fields DataFrame.\n",
    "########### SciSciNet_Fields.tsv\n",
    "Fields_df = read_big_csv(\"%s/data/raw/SciSciNet_Fields.tsv\"%pre_path, sep='\\t', compression=None, chunksize=1000000, nrows=None, \n",
    "                           usecols=['FieldID','Field_Name','Field_Type'])\n",
    "Fields_df_top = Fields_df[Fields_df['Field_Type'] == 'Top'] # Filter to keep only the top fields.\n",
    "\n",
    "Paper_Fields_df_top = pd.merge(Paper_Fields_df, Fields_df_top, how='inner', on=['FieldID'])\n",
    "Paper_Fields_dict = Paper_Fields_df_top.set_index(\"PaperNewID\")[\"Field_Name\"].to_dict()\n",
    "pickle.dump(Paper_Fields_dict, open(\"%s/data/processed/PaperID_TopField.pickle\"%pre_path, 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b0bc0",
   "metadata": {},
   "source": [
    "#### Paper journal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extract the SJR journal rank of papers recorded in the SciSciNet dataset \"SciSciNet_Papers.tsv\".\n",
    "'''\n",
    "\n",
    "# Extract the ISSN-ranking data from Scimago Journal Rank (SJR) for each year (1999-2024).\n",
    "ISSN_rank = defaultdict(dict)\n",
    "for year in range(1999,2025):\n",
    "    df = pd.read_csv('%s/data/raw/scimagojr/scimagojr_%s.csv'%(pre_path,year), sep=';')\n",
    "    df = df[df['SJR Best Quartile'] != '-'] # Filter out rows where 'SJR Best Quartile' is '-'.\n",
    "    \n",
    "    ISSNs_list = df['Issn'].tolist()\n",
    "    ranks_list = df['SJR Best Quartile'].tolist() # ['Q1','Q2','Q3','Q4']\n",
    "    \n",
    "    for i in range(len(ISSNs_list)):\n",
    "        ISSNs = ISSNs_list[i].split(', ') # Split ISSNs if there are multiple ISSNs in the same cell.\n",
    "        rank = ranks_list[i]\n",
    "        for ISSN in ISSNs:\n",
    "            issn = ISSN[:4]+'-'+ISSN[4:]  # Format the ISSN to have a hyphen between the 4th and 5th characters.\n",
    "            if issn not in ISSN_rank:\n",
    "                ISSN_rank[issn] = defaultdict(str)\n",
    "            ISSN_rank[issn][year] = rank  # Store the rank for each ISSN for the corresponding year.\n",
    "pickle.dump(ISSN_rank, open(\"%s/data/processed/ISSN_year_rank.pickle\"%pre_path, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# Load the PaperID-Year dict from the pickle file.\n",
    "paper_year = pickle.load(open('/scratch/xy2949/SciSciNet_datasets/pickles/PaperID_Year.pickle', 'rb'))\n",
    "paper_year = pd.DataFrame(list(paper_year.items()), columns=['PaperID', 'year'])\n",
    "\n",
    "\n",
    "# Load the SciSciNet_Papers.tsv file and extract the PaperID and JournalID.\n",
    "Paper_Journal = read_big_csv(\"%s/data/raw/SciSciNet_Papers.tsv\"%pre_path, sep='\\t', compression=None, chunksize=1000000, nrows=None, \n",
    "                           usecols=['PaperID','JournalID'])\n",
    "\n",
    "# Load the SciSciNet_Journals.tsv file and extract the JournalID and ISSN.\n",
    "Journal_ISSN = read_big_csv(\"%s/data/raw/SciSciNet_Journals.tsv\"%pre_path, sep='\\t', compression=None, chunksize=1000000, nrows=None, \n",
    "                           usecols=['JournalID','ISSN'])\n",
    "\n",
    "# Merge the Paper_Journal and Journal_ISSN DataFrames to get the ISSN for each PaperID.\n",
    "Paper_ISSN = pd.merge(Paper_Journal, Journal_ISSN, how='inner', on = 'JournalID')\n",
    "# Merge Paper_ISSN with paper_year to get the year for each PaperID.\n",
    "Paper_ISSN_Year = pd.merge(Paper_ISSN, paper_year, how='inner', on = 'PaperID')\n",
    "# column: ['PaperID','year','JournalID','ISSN']\n",
    "Paper_ISSN_Year = Paper_ISSN_Year[['PaperID','year','ISSN']]\n",
    "del Paper_Journal,Journal_ISSN,paper_year,Paper_ISSN\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Load the ISSN-year-rank data from the pickle file and merge it with Paper_ISSN_Year.\n",
    "ISSN_Year_Rank_Dict = pickle.load(open(\"%s/data/processed/ISSN_year_rank.pickle\"%pre_path, \"rb\"))\n",
    "ISSN_Year_Rank = pd.DataFrame(\n",
    "    [(issn, year, rank) for issn, years in ISSN_Year_Rank_Dict.items() for year, rank in years.items()],\n",
    "    columns=['ISSN', 'year', 'rank']\n",
    ")\n",
    "'''\n",
    "ISSN_list, Year_list, Rank_list = [], [], []\n",
    "for issn in ISSN_Year_Rank_Dict:\n",
    "    for year in ISSN_Year_Rank_Dict[issn]:\n",
    "        rank = ISSN_Year_Rank_Dict[issn][year]\n",
    "        \n",
    "        ISSN_list.append(issn)\n",
    "        Year_list.append(year)\n",
    "        Rank_list.append(rank)\n",
    "ISSN_Year_Rank = pd.DataFrame({'ISSN':ISSN_list, 'year':Year_list, 'rank':Rank_list})\n",
    "'''\n",
    "Paper_ISSN_Rank = pd.merge(Paper_ISSN_Year, ISSN_Year_Rank, how='inner', on = ['ISSN','year'])\n",
    "del ISSN_Year_Rank_Dict, Paper_ISSN_Year, ISSN_Year_Rank#, ISSN_list, Year_list, Rank_list\n",
    "gc.collect()\n",
    "\n",
    "# Create a dictionary mapping PaperID to its ISSN rank.\n",
    "Paper_rank = Paper_ISSN_Rank[[\"PaperID\", \"rank\"]].set_index(\"PaperID\").to_dict()[\"rank\"]\n",
    "pickle.dump(Paper_rank, open(\"%s/data/processed/PaperID_ISSN_SJR.pickle\"%(pre_path), 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b42e07",
   "metadata": {},
   "source": [
    "## Calculating the Knowledge Independence (KI) of papers\n",
    "Intuitively, for any given paper, the measure quantifies the degree to which its references are \"independent\" in the sense that they do not cite one another. To this end, let us introduce two types of references: An $i$-type reference is one that does not cite any other work within the same reference list, whereas a $j$-type reference is one that does cite at least one other work in this list. Then, knowledge independence (KI) is measured as the difference between the fraction of $i$-type and $j$-type references. More formally:\n",
    "\n",
    "$$\n",
    "{\\rm{KI}} = \\frac{n_i-n_j}{n_i+n_j}. \\quad\\quad(1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the citation_matrix_csr.\n",
    "citation_matrix_csr = pickle.load(open(\"%s/data/processed/citation_matrix_csr.pickle\"%pre_path, \"rb\")) # efficient row slicing (reference list)\n",
    "csr_indptr = citation_matrix_csr.indptr # an array that storing the starting index of each citing paper's references in the indices list.\n",
    "csr_indices = citation_matrix_csr.indices # an array that storing the indices of the cited papers in the citation matrix.\n",
    "del citation_matrix_csr\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e11b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the KI and reference count of each paper. \n",
    "# Here we calculate four different versions of KI:\n",
    "# 1. KI2: the principal definition used in the main text, with reference list no less than 2, otherwise the KI is bound to be 1 by definition.\n",
    "# 2. KI2_frac: simplifying the principal equation by only measuring the fraction of i-type references.\n",
    "# 3. KI2_adj: subtracting 1 from the count of i-type references, to account for the oldest reference in the bibliography, which cannot cite any other reference in that list.\n",
    "# 4. KI2_adj_frac: simplifing the previous equation by only measuring the fraction of i-type reference.\n",
    "paper_KI2 = defaultdict(float)\n",
    "paper_KI2_frac = defaultdict(float)\n",
    "paper_KI2_adj = defaultdict(float)\n",
    "paper_KI2_adj_frac = defaultdict(float)\n",
    "paper_reference_Count = defaultdict(int) # reference list length of each paper.\n",
    "\n",
    "\n",
    "max_idx = len(csr_indptr) - 1\n",
    "# i = 0\n",
    "for ix in range(max_idx):\n",
    "    # i += 1\n",
    "    # if i%1000000 == 0:\n",
    "        # print('Processed %s papers'%i)\n",
    "    a = csr_indptr[ix]\n",
    "    b = csr_indptr[ix+1]\n",
    "    reference_set_ix = set(csr_indices[a:b]) # reference list of the focal paper ix.\n",
    "    L = len(reference_set_ix)\n",
    "    paper_reference_Count[ix] = L\n",
    "    if L < 2: # Skip papers with less than 2 references.\n",
    "        continue\n",
    "    \n",
    "    # identify the j-type references in the reference list of the focal paper ix, which ever cites any other reference.\n",
    "    n_j = 0\n",
    "    for iz in reference_set_ix:\n",
    "        aa = csr_indptr[iz]\n",
    "        bb = csr_indptr[iz+1]\n",
    "        reference_set_iz = set(csr_indices[aa:bb])\n",
    "        if not reference_set_ix.isdisjoint(reference_set_iz):  # isdisjoint: Whether two sets are \"disjoint\"\n",
    "            n_j += 1\n",
    "    n_i = L - n_j # count of i-type references in the reference list of the focal paper ix.\n",
    "    \n",
    "    # KI measure\n",
    "    paper_KI2[ix] = (n_i - n_j) / L\n",
    "    paper_KI2_frac[ix] = n_i / L\n",
    "    paper_KI2_adj[ix] = (n_i - 1 - n_j) / (L - 1)\n",
    "    paper_KI2_adj_frac[ix] = (n_i - 1) / (L - 1)\n",
    "\n",
    "pickle.dump(paper_KI2, open(\"%s/data/processed/PaperID_KI2.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(paper_KI2_frac, open(\"%s/data/processed/PaperID_KI2_frac.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(paper_KI2_adj, open(\"%s/data/processed/PaperID_KI2_adj.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(paper_KI2_adj_frac, open(\"%s/data/processed/PaperID_KI2_adj_frac.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(paper_reference_Count, open(\"%s/data/processed/PaperID_reference_Count.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05e034",
   "metadata": {},
   "source": [
    "## Calculating the Disruption index of papers\n",
    "the disruption index is calculated as $D=\\dfrac{c_i-c_j}{c_i+c_j+c_k}$, where $c_i$ represents the number of works that only cite the focal work, $c_j$ represents the number of works that cite both the focal work and its references, and $c_k$ represents the number of subsequent works that cite the references of the focal work but not the focal work itself. There has been ongoing debate on the rationale behind the inclusion of the term $c_k$ in the formula. Specifically, $c_k$ quantifies the attention directed towards the focal work's references while bypassing the focal work itself. Under fixed $c_i$ and $c_j$, focal works with higher $c_k$ values would be considered less disruptive. However, a contradiction arises in cases of negative $D$, where an increase in $c_k$ paradoxically increases $D$ rather than decreasing it---contrary to the conceptual definition of disruption. To address this inconsistency, we adopt a modified definition of disruption that excludes the term $c_k$. This modified measure serves as the principal metric of disruption in our study, while the original definition is retained as an alternative for robustness check. Additionally, we implement an open time window to count the citations a focal work receives ($c^{o}$) and use a 5-year time window ($c^{5}$) as an alternative measure.\n",
    "\n",
    "The disruption measures that we consider are defined as follows: $D_0$ is the principal definition used in the main text. $D_1$ takes the form of original definition with an open time window $c^{o}$. $D_2$ is defined the same way as $D_0$, but with a different time window $c^{5}$. $D_3$ takes the form of original definition with a 5-year time window $c^{5}$. More formally, these measures are defined as follows:\n",
    "\n",
    "$D_0=\\dfrac{c^{o}_i-c^{o}_j}{c^{o}_i+c^{o}_j}$, $D_1=\\dfrac{c^{o}_i-c^{o}_j}{c^{o}_i+c^{o}_j+c^{o}_k}$, $D_2=\\dfrac{c^{5}_i-c^{5}_j}{c^{5}_i+c^{5}_j}$, $D_3=\\dfrac{c^{5}_i-c^{5}_j}{c^{5}_i+c^{5}_j+c^{5}_k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc499d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the paper_year, the citation_matrix_csr and transformed citation_matrix_csr.\n",
    "paper_year = pickle.load(open('%s/data/processed/PaperID_Year.pickle'%pre_path, 'rb'))\n",
    "citation_matrix_csr = pickle.load(open(\"%s/data/processed/citation_matrix_csr.pickle\"%pre_path, \"rb\")) # efficient row slicing (reference list)\n",
    "csr_indptr = citation_matrix_csr.indptr # an array that storing the starting index of each citing paper's references in the indices list.\n",
    "csr_indices = citation_matrix_csr.indices # an array that storing the indices of the cited papers in the citation matrix.\n",
    "citation_matrix_csc = citation_matrix_csr.tocsc() # efficient column slicing (received citations)\n",
    "csc_indptr = citation_matrix_csc.indptr # an array that storing the starting index of each paper's received citations in the indices list.\n",
    "csc_indices = citation_matrix_csc.indices # an array that storing the indices of the citing papers in the citation matrix.\n",
    "del citation_matrix_csr,citation_matrix_csc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63501b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Disruption and citation count of each paper. \n",
    "# Here we calculate four different versions of Disruption:\n",
    "paper_D5 = defaultdict(float)\n",
    "paper_D5_nok = defaultdict(float)\n",
    "paper_Dopen = defaultdict(float)\n",
    "paper_Dopen_nok = defaultdict(float)\n",
    "Paper_C5, Paper_Copen = defaultdict(int), defaultdict(int) # Count the number of received citations with open and 5-year citation window.\n",
    "\n",
    "citation_window = 5\n",
    "max_id = len(csr_indptr) - 1\n",
    "# i = 0\n",
    "for ix in range(max_id):\n",
    "    # i += 1\n",
    "    # if i%100000 == 0:\n",
    "        # print(i)\n",
    "    if ix not in paper_year: continue # Skip papers without year information.\n",
    "    year_ix = paper_year[ix]\n",
    "\n",
    "    # Get the set of received citations for the focal paper ix. \n",
    "    a1 = csc_indptr[ix]\n",
    "    b1 = csc_indptr[ix+1]\n",
    "    citation_set_ix = set(csc_indices[a1:b1])\n",
    "    L = len(citation_set_ix) # Count the number of received citations with open citation window.\n",
    "    Paper_Copen[ix] = L\n",
    "\n",
    "    L5 = 0 # Count the number of received citations within the citation window.\n",
    "    for iy in citation_set_ix:\n",
    "        if iy in paper_year and paper_year[iy] <= year_ix+citation_window:\n",
    "            L5 += 1\n",
    "    Paper_C5[ix] = L5\n",
    "\n",
    "    if L < 1: continue # Skip papers with no received citations.\n",
    "\n",
    "    # Get the set of references for the focal paper ix.\n",
    "    a2 = csr_indptr[ix]\n",
    "    b2 = csr_indptr[ix+1]\n",
    "    reference_set_ix = set(csr_indices[a2:b2])\n",
    "    if len(reference_set_ix) < 1: continue # Skip papers with no references.\n",
    "    \n",
    "    # Build a list of citations received by all references in the reference list of the focal paper ix.\n",
    "    citation_list_iz = []\n",
    "    for iz in reference_set_ix:\n",
    "        a3 = csc_indptr[iz]\n",
    "        b3 = csc_indptr[iz+1]\n",
    "        citation_list_iz += list(csc_indices[a3:b3])\n",
    "    citation_set_iz = set(citation_list_iz)\n",
    "\n",
    "\n",
    "    n_j, n_k = 0, 0\n",
    "    n_j_5, n_k_5 = 0, 0\n",
    "    for izz in citation_set_iz:\n",
    "        if izz in citation_set_ix: # Check if the citation (izz) of reference (iz) also cites the focal paper ix.\n",
    "            n_j += 1 # Count the number of j-type citation that cite both the focal paper ix and its references.\n",
    "            if izz in paper_year and paper_year[izz] <= year_ix+citation_window:\n",
    "                n_j_5 += 1\n",
    "        else: # if the citation (izz) of reference (iz) does not cite the focal paper ix, and published after paper ix, it is k-type citation.\n",
    "            if izz in paper_year and paper_year[izz] > year_ix:\n",
    "                n_k += 1\n",
    "                if paper_year[izz] <= year_ix+citation_window:\n",
    "                    n_k_5 += 1\n",
    "\n",
    "    # D measure\n",
    "    n_i = L - n_j\n",
    "    paper_Dopen[ix] = (n_i - n_j) / (L + n_k)\n",
    "    paper_Dopen_nok[ix] = (n_i - n_j) / L\n",
    "    \n",
    "    if L5 < 1: continue\n",
    "    n_i_5 = L5 - n_j_5\n",
    "    paper_D5[ix] = (n_i_5 - n_j_5) / (L5 + n_k_5)\n",
    "    paper_D5_nok[ix] = (n_i_5 - n_j_5) / L5\n",
    "\n",
    "pickle.dump(paper_Dopen, open(\"%s/data/processed/PaperID_Dopen.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(paper_Dopen_nok, open(\"%s/data/processed/PaperID_Dopen_nok.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(paper_D5, open(\"%s/data/processed/PaperID_D5.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(paper_D5_nok, open(\"%s/data/processed/PaperID_D5_nok.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(Paper_Copen, open(\"%s/data/processed/PaperID_Copen.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(Paper_C5, open(\"%s/data/processed/PaperID_C5.pickle\"%pre_path, 'wb'),pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f820245",
   "metadata": {},
   "source": [
    "## Paper reference properties\n",
    "Extract the statistical patterns of focal paper's references, such as average age, average impact, and average\n",
    "disruption of references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd80e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the citation_matrix_csr and interested paper properties.\n",
    "citation_matrix_csr = pickle.load(open(\"%s/data/processed/citation_matrix_csr.pickle\"%pre_path, \"rb\")) # efficient row slicing (reference list)\n",
    "csr_indptr = citation_matrix_csr.indptr # an array that storing the starting index of each citing paper's references in the indices list.\n",
    "csr_indices = citation_matrix_csr.indices # an array that storing the indices of the cited papers in the citation matrix.\n",
    "del citation_matrix_csr\n",
    "gc.collect()\n",
    "\n",
    "# paper properties\n",
    "paper_C5 = pickle.load(open('%s/data/processed/PaperID_C5.pickle'%pre_path, 'rb'))\n",
    "paper_Dopen_nok = pickle.load(open('%s/data/processed/PaperID_Dopen_nok.pickle'%pre_path, 'rb'))\n",
    "paper_year = pickle.load(open('%s/data/processed/PaperID_Year.pickle'%pre_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper reference properties\n",
    "paper_reference_C5 = defaultdict(float) # average C5 of the references of the focal paper.\n",
    "paper_reference_Dopen_nok = defaultdict(float) # average Dopen_nok of the references of the focal paper.\n",
    "paper_reference_age = defaultdict(float) # average age of the references of the focal paper, defined as the difference between the year of the focal paper and the year of the reference.\n",
    "\n",
    "max_id = len(csr_indptr) - 1\n",
    "# i = 0\n",
    "for ix in range(max_id):\n",
    "    # i += 1\n",
    "    # if i%100000 == 0:\n",
    "        # print(i)\n",
    "    a = csr_indptr[ix]\n",
    "    b = csr_indptr[ix+1]\n",
    "    reference_set_ix = set(csr_indices[a:b]) # reference list of the focal paper ix.\n",
    "    L = len(reference_set_ix)\n",
    "    if L == 0:  # Skip papers with no references.\n",
    "        continue\n",
    "    \n",
    "    sum_C5, sum_Dopen_nok, sum_age = 0.0, 0.0, 0.0\n",
    "    count_C5, count_Dopen_nok, count_age = 0, 0, 0\n",
    "    for iy in reference_set_ix:        \n",
    "        if iy in paper_C5:\n",
    "            sum_C5 += paper_C5[iy]\n",
    "            count_C5 += 1\n",
    "        if iy in paper_Dopen_nok:\n",
    "            sum_DC += paper_Dopen_nok[iy]\n",
    "            count_DC += 1\n",
    "        if ix in paper_year and iy in paper_year:\n",
    "            sum_age += (paper_year[ix]-paper_year[iy])\n",
    "            count_age += 1\n",
    "        \n",
    "    if count_C5 > 0:\n",
    "        paper_reference_C5[ix] = sum_C5/count_C5\n",
    "    if count_DC > 0:\n",
    "        paper_reference_Dopen_nok[ix] = sum_DC/count_DC\n",
    "    if count_age > 0:\n",
    "        paper_reference_age[ix] = sum_age/count_age\n",
    "del csr_indptr,csr_indices,paper_C5,paper_Dopen_nok,paper_year\n",
    "gc.collect()\n",
    "\n",
    "pickle.dump(paper_reference_C5, open(\"%s/data/processed/PaperID_reference_C5.pickle\"%pre_path, 'wb'), pickle.HIGHEST_PROTOCOL)   \n",
    "pickle.dump(paper_reference_Dopen_nok, open(\"%s/data/processed/PaperID_reference_Dopen_nok.pickle\"%pre_path, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(paper_reference_age, open(\"%s/data/processed/PaperID_reference_age.pickle\"%pre_path, 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6569f01",
   "metadata": {},
   "source": [
    "## Paper authorship properties\n",
    "#### 1) Paper author sequence\n",
    "For each paper, obtain its ahtuor sequence according to the author position. \n",
    "\n",
    "#### 2) Author paper sequence\n",
    "For each author, obtain his/her paper sequence during the whole recorded career according to the publication date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e91bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the SciSciNet_PaperAuthorAffiliations.tsv file to extract the authorship of papers.\n",
    "########### SciSciNet_PaperAuthorAffiliations.tsv\n",
    "property_list = ['PaperID', 'AuthorID', 'AuthorSequenceNumber']\n",
    "Papers_Author_df = read_big_csv(\"%s/data/raw/SciSciNet_PaperAuthorAffiliations.tsv\"%pre_path, sep='\\t', compression=None, chunksize=1000000, nrows=None, \n",
    "                           usecols=property_list)\n",
    "Papers_Author_df.dropna(subset=['PaperID', 'AuthorID'],inplace=True)\n",
    "Papers_Author_df = Papers_Author_df.drop_duplicates(subset=['PaperID', 'AuthorID']) # keep the first occurrence\n",
    "\n",
    "\n",
    "# sort the Papers_Author_df by PaperID and AuthorSequenceNumber.\n",
    "Papers_Author_df_sorted = Papers_Author_df.sort_values(by=['PaperID', 'AuthorSequenceNumber'], ascending=[False,True])\n",
    "Papers_Author_df_sorted.reset_index(drop = True,inplace = True)\n",
    "\n",
    "paper_authors_all = {} # paper_authors_all is a dictionary that stores the authors and their positions for each paper.\n",
    "paper_list = list(Papers_Author_df_sorted['PaperID'].drop_duplicates()) # get the unique paper IDs\n",
    "paper_counter = Counter(Papers_Author_df_sorted['PaperID'].tolist())    # Count the number of authors for each paper.\n",
    "\n",
    "end_tag = 0\n",
    "for paper in paper_list:\n",
    "    paper_authors_all[paper] = {}\n",
    "    start_tag = end_tag\n",
    "    end_tag = start_tag + paper_counter[paper]\n",
    "    df_paper = Papers_Author_df_sorted[start_tag:end_tag]\n",
    "    if not isinstance(df_paper, pd.DataFrame): continue # Check if df_paper is a DataFrame.\n",
    "    authors = df_paper['AuthorID'].tolist()\n",
    "    author_positions = df_paper['AuthorSequenceNumber'].tolist()\n",
    "    # Convert the author positions to a more readable format.\n",
    "    # for i,position in enumerate(author_positions):\n",
    "        # if len(authors) == 1:\n",
    "            # author_positions[i] = 's'\n",
    "        # else:\n",
    "            # if position == 'first':\n",
    "                # author_positions[i] = 'f'\n",
    "            # elif position == 'last':\n",
    "                # author_positions[i] = 'l'\n",
    "            # else:\n",
    "                # author_positions[i] = 'o'\n",
    "    paper_authors_all[paper]['author_sequence'] = authors\n",
    "    paper_authors_all[paper]['position_sequence'] = author_positions\n",
    "del paper_list,paper_counter,Papers_Author_df_sorted\n",
    "gc.collect()\n",
    "pickle.dump(paper_authors_all, open(\"%s/data/processed/Paper_Author_Position_Sequence.pickle\"%pre_path, 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fe0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to datetime format and drop rows with NaN values.\n",
    "paper_date = pickle.load(open(\"%s/data/processed/PaperID_Date.pickle\"%pre_path, 'rb'))\n",
    "Papers_Date_df = pd.DataFrame(list(paper_date.items()), columns=['PaperID', 'Date'])\n",
    "del paper_date\n",
    "gc.collect()\n",
    "\n",
    "Papers_Date_df['Date'] = pd.to_datetime(Papers_Date_df['Date'])\n",
    "Papers_Date_df.dropna(inplace=True)\n",
    "Papers_Date_df = Papers_Date_df.drop_duplicates() # keep the first occurrence\n",
    "\n",
    "# Merge the Papers_Date_df and Papers_Author_df to get the date of each paper and its authors.\n",
    "Papers_Date_Author_df = pd.merge(Papers_Date_df, Papers_Author_df, how='inner', on = 'PaperID')\n",
    "del Papers_Date_df,Papers_Author_df\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Sort the Papers_Date_Author_df by AuthorID and Date, and reset the index.\n",
    "Papers_Date_Author_df = Papers_Date_Author_df.sort_values(by=[\"AuthorID\", \"Date\"], ascending=[False,True]) # sorted by date and doi, old to new\n",
    "Papers_Date_Author_df.reset_index(drop = True,inplace = True) # 默认索引值0-n为索引，且原索引既不作为数据值存在，也不作为索引存在\n",
    "\n",
    "author_papers_all = {} # author_papers_all is a dictionary that stores the sequenced papers and their positions for each author.\n",
    "author_list = list(Papers_Date_Author_df['AuthorID'].drop_duplicates()) # get the unique author IDs\n",
    "author_counter = Counter(Papers_Date_Author_df['AuthorID'].tolist()) # Count the number of papers for each author.\n",
    "\n",
    "end_tag = 0\n",
    "for author in author_list:\n",
    "    author_papers_all[author] = {}\n",
    "    start_tag = end_tag\n",
    "    end_tag = start_tag + author_counter[author]\n",
    "    df_author = Papers_Date_Author_df[start_tag:end_tag]\n",
    "    if not isinstance(df_author, pd.DataFrame): continue # Check if df_author is a DataFrame.\n",
    "    papers = df_author['PaperID'].tolist()\n",
    "    author_positions = df_author['AuthorSequenceNumber'].tolist()\n",
    "    # Convert the author positions to a more readable format.\n",
    "    # for i,position in enumerate(author_positions):\n",
    "        # if author_count[i] == 1:\n",
    "            # author_positions[i] = 's' # single author\n",
    "        # else:\n",
    "            # if position == 'first':\n",
    "                # author_positions[i] = 'f'\n",
    "            # elif position == 'last':\n",
    "                # author_positions[i] = 'l'\n",
    "            # else:\n",
    "                # author_positions[i] = 'o'\n",
    "    author_papers_all[author]['paper_sequence'] = papers\n",
    "    author_papers_all[author]['position_sequence'] = author_positions\n",
    "pickle.dump(author_papers_all, open(\"%s/data/processed/Author_Paper_Position_Sequence.pickle\"%pre_path, 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c6b5bc",
   "metadata": {},
   "source": [
    "## Team compositions of papers\n",
    "We measure team size, collaborative freshness, and geographic distance of the focal paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d4e9d",
   "metadata": {},
   "source": [
    "#### Team size and freshness\n",
    "To quantify team freshness, we analyze the prior collaboration network of the authors involved in a given focal paper. In this network, the undirected edges represent prior collaborations between pairs of authors before coauthoring the focal paper. We classify team freshness into mutually exclusive categories based on the topological structure of the collaboration network, where $V$, $k$, and $n$ represent the node set, node degree, and network size, respectively. More specifically, the classification is as follows:\n",
    "- $0$: All authors have previously collaborated with each other, $\\{\\forall v_i \\in V, \\ k_i = n-1\\}$.\n",
    "- $1$: All authors have at least one prior collaboration link, $\\{\\forall v_i \\in V, 1 \\leq \\ k_i \\leq n-1\\}$.\n",
    "- $2$: Some authors have at least one prior collaboration link, $\\{\\exists v_i \\in V, \\ k_i = 0\\}$.\n",
    "- $3$: No author has any prior collaboration link, $\\{\\forall v_i \\in V, \\ k_i = 0\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa89c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "author_papers = pickle.load(open(\"%s/data/processed/Author_Paper_Position_Sequence.pickle\"%pre_path, 'rb'))\n",
    "paper_authors = pickle.load(open(\"%s/data/processed/Paper_Author_Position_Sequence.pickle\"%pre_path, 'rb'))\n",
    "paper_date = pickle.load(open(\"%s/data/processed/PaperID_Date.pickle\"%pre_path, 'rb'))\n",
    "paper_list = list(paper_date.keys())\n",
    "del paper_date\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750519a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the team size and freshness of each paper.\n",
    "# Team freshness in our definition is \n",
    "team_size = defaultdict(int) # team_size is a dictionary that stores the number of authors in each paper.\n",
    "team_freshness = defaultdict(int) # team_freshness is a dictionary that stores the freshness of each paper.\n",
    "###############################################\n",
    "# i = 0\n",
    "for ix in paper_list:\n",
    "    # i += 1\n",
    "    # if i%1000000 == 0:\n",
    "    #     print('Processed %s papers'%i)\n",
    "    if ix not in paper_authors: continue # Skip papers without authors.\n",
    "    authors_set_ix = set(paper_authors[ix]['author_sequence'])\n",
    "    teamsize_ix = len(authors_set_ix)\n",
    "    team_size[ix] = teamsize_ix # Record the number of authors in the paper.\n",
    "    if teamsize_ix < 2: continue # Skip papers with less than 2 authors in measuring collaborative freshness.\n",
    "    \n",
    "    error_tag = False # a flag to indicate if any author has no papers.\n",
    "    former_papers_set = {} # former_papers_set is a dictionary that stores the set of papers authored by each author before the focal paper ix.\n",
    "    for author in authors_set_ix:\n",
    "        if author not in author_papers: # If the author has no papers, skip this author.\n",
    "            error_tag = True\n",
    "            break\n",
    "        papers_list_a = author_papers[author]['paper_sequence'] # Get the sequenced list of papers authored by this author.\n",
    "        ix_idx = papers_list_a.index(ix) # Get the index of the focal paper ix in this sequenced paper list.\n",
    "        former_papers_set[author] = set(papers_list_a[:ix_idx]) # Get the set of papers authored by this author before the focal paper ix.\n",
    "    if error_tag: continue # If any author has no papers, skip this paper.\n",
    "    \n",
    "    author_pairs = combinations(authors_set_ix,2) # Get all possible pairs of authors in the paper.\n",
    "    old_pair_count_true = 0 # Count the number of old collaborated pairs of authors in the paper.\n",
    "    old_pair_count_max = len(authors_set_ix)*(len(authors_set_ix)-1)/2 # Maximum number of old collaborated pairs of authors in the paper.\n",
    "        \n",
    "    for author_pair in author_pairs:\n",
    "        author_A, author_B = author_pair\n",
    "        paper_set_A, paper_set_B = former_papers_set[author_A], former_papers_set[author_B]\n",
    "        if len(paper_set_A) < len(paper_set_B):\n",
    "            small_set, large_set = paper_set_A, paper_set_B\n",
    "        else:\n",
    "            small_set, large_set = paper_set_B, paper_set_A\n",
    "        for paper in small_set:\n",
    "            if paper in large_set: # If the paper is authored by both authors, it is an old collaborated pair.\n",
    "                old_pair_count_true += 1\n",
    "                authors_set_ix.discard(author_A) # Remove the author from the set of authors in the paper for the ease to record fresh authors.\n",
    "                authors_set_ix.discard(author_B)\n",
    "                break\n",
    "    # fresh by category\n",
    "    if old_pair_count_true == 0:   # no author have any old collaborated pair\n",
    "        team_freshness[ix] = 3\n",
    "    elif old_pair_count_true == old_pair_count_max: # all authors have collaborated with each other\n",
    "        team_freshness[ix] = 0\n",
    "    elif len(authors_set_ix) == 0: # all authors have at least one old collaborated pair\n",
    "        team_freshness[ix] = 1\n",
    "    else:                          # some authors have at least one old collaborated pair\n",
    "        team_freshness[ix] = 2\n",
    "\n",
    "del author_papers, paper_authors, paper_list\n",
    "gc.collect()\n",
    "\n",
    "pickle.dump(team_size, open(\"%s/data/processed/PaperID_Team_Size.pickle\"%pre_path, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(team_freshness, open(\"%s/data/processed/PaperID_Team_Freshness.pickle\"%pre_path, 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419151c1",
   "metadata": {},
   "source": [
    "#### Team distance (km)\n",
    "1) We first calculate the distance between each ever collaborated affiliationID pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance between each ever collaborated affiliationID pair\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6385.0  # Radius of the Earth (km)\n",
    "\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d7ed9",
   "metadata": {},
   "source": [
    "1.1 load the SciSciNet_PaperAuthorAffiliations.tsv file to extract the distinct affiliation pairs of papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8537f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the SciSciNet_PaperAuthorAffiliations.tsv file to extract the distinct affiliation pairs of papers.\n",
    "# column: [PaperID, AuthorID, AffiliationID, AuthorSequenceNumber]\n",
    "Paper_Affs_df = read_big_csv(\"%s/data/raw/SciSciNet_PaperAuthorAffiliations.tsv\"%pre_path, sep='\\t', compression=None, chunksize=1000000, nrows=None, \n",
    "                           usecols=['PaperID', 'AffiliationID'])\n",
    "\n",
    "# count the number of affiliations for each paper\n",
    "Paper_Affs_df['PaperCount'] = Paper_Affs_df.groupby('PaperID')['PaperID'].transform('count')\n",
    "# select papers with more than one affiliation\n",
    "paper_multi_aff_df = Paper_Affs_df[Paper_Affs_df['PaperCount']>1]\n",
    "paper_multi_aff_df = paper_multi_aff_df.drop(columns=['PaperCount'])\n",
    "\n",
    "# for multiple affiliation PaperID, select rows where 'AffiliationID' is NaN (in case 'NAN' becomes a ghost AffiliationID)\n",
    "nan_affiliation_paper_ids = set(paper_multi_aff_df[paper_multi_aff_df['AffiliationID'].isna()]['PaperID'].tolist())\n",
    "\n",
    "# remove multiple affiliation PaperID with NaN AffiliationID from paper_multi_aff_df\n",
    "paper_multi_aff_df = paper_multi_aff_df[~paper_multi_aff_df['PaperID'].isin(nan_affiliation_paper_ids)]\n",
    "del Paper_Affs_df,nan_affiliation_paper_ids\n",
    "gc.collect()\n",
    "\n",
    "# sorted by PaperID and AffiliationID\n",
    "paper_multi_aff_df['AffiliationID'] = paper_multi_aff_df['AffiliationID'].astype(int)\n",
    "paper_multi_aff_df = paper_multi_aff_df.sort_values(by=['PaperID','AffiliationID'], ascending=[True,True])\n",
    "paper_multi_aff_df.reset_index(drop=True, inplace=True)\n",
    "###################################\n",
    "\n",
    "paper_list = list(paper_multi_aff_df['PaperID'].drop_duplicates()) # get the unique paper IDs\n",
    "paper_aff_counter = Counter(paper_multi_aff_df['PaperID'].tolist()) # Count the number of affiliations for each paper.\n",
    "\n",
    "aff_list_1, aff_list_2 = [], [] # Initialize empty lists to store collaborated pair of affiliation ID (1: the smaller ID, 2: the larger ID).\n",
    "end_tag = 0\n",
    "for paper in paper_list:\n",
    "    start_tag = end_tag\n",
    "    end_tag = start_tag + paper_aff_counter[paper]\n",
    "    df_paper = paper_multi_aff_df[start_tag:end_tag]\n",
    "    if not isinstance(df_paper, pd.DataFrame): continue # Check if df_paper is a DataFrame.\n",
    "    aff_list = list(df_paper['AffiliationID'].drop_duplicates()) # get the sorted unique affiliation IDs for the paper.\n",
    "\n",
    "    distinc_aff_count = len(aff_list)\n",
    "    if distinc_aff_count < 2: continue # Skip papers with less than 2 distinct affiliations.\n",
    "    \n",
    "    for i in range(distinc_aff_count):\n",
    "        aff_list_1 += aff_list[i:i+1]*(distinc_aff_count-i) # append the current affiliation ID to aff_list_1, repeated (distinc_aff_count-i) times.\n",
    "        aff_list_2 += aff_list[i:distinc_aff_count] # append the remaining affiliation IDs to aff_list_2, starting from the current index i.\n",
    "AffID_pair_DF = pd.DataFrame({'affid_1':aff_list_1,'affid_2':aff_list_2})\n",
    "AffID_pair_DF = AffID_pair_DF.drop_duplicates(subset=['affid_1','affid_2'], keep='first') # keep the first occurrence of each pair.\n",
    "AffID_pair_DF.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "\n",
    "# get the unique pairs of affiliation IDs (1: the smaller ID, 2: the larger ID)\n",
    "aff_list_1 = list(AffID_pair_DF['affid_1'])\n",
    "aff_list_2 = list(AffID_pair_DF['affid_2'])\n",
    "del paper_list,paper_aff_counter,paper_multi_aff_df,AffID_pair_DF\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5f738",
   "metadata": {},
   "source": [
    "1.2 Load the SciSciNet_Affiliations.tsv file to extract the coordinates of affiliations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SciSciNet_Affiliations.tsv file to extract the coordinates of affiliations.\n",
    "# column: [AffiliationID, Affiliation_Name, GridID, Official_Page,ISO3166Code,Latitude,Longitude,H-index,Productivity,Average_C10,Average_LogC10]\n",
    "Aff_coordinate_df = pd.read_csv(\"%s/data/raw/SciSciNet_Affiliations.tsv\"%pre_path, sep='\\t', compression=None, nrows=None, \n",
    "                           usecols=['AffiliationID', 'Latitude', 'Longitude'])\n",
    "Aff_coordinate_df.dropna(axis=0, how='any', inplace=True)\n",
    "Aff_coordinate_df = Aff_coordinate_df.drop_duplicates(subset=['AffiliationID'], keep='first')\n",
    "Aff_coordinate_df['AffiliationID'] = Aff_coordinate_df['AffiliationID'].astype(int)\n",
    "\n",
    "# Create a dictionary to store the coordinates of each affiliation.\n",
    "aff_coordinate_dict = Aff_coordinate_df.set_index('AffiliationID')[['Latitude', 'Longitude']].apply(tuple, axis=1).to_dict()\n",
    "del Aff_coordinate_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf1c5c",
   "metadata": {},
   "source": [
    "1.3 Calculate the distance between each ever collaborated affiliationID pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7bdbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between each ever collaborated affiliationID pair\n",
    "Aff_pair_distance = defaultdict(float) # Aff_pair_distance is a dictionary that stores the distance between each ever collaborated affiliationID pair.\n",
    "\n",
    "for i in range(len(aff_list_1)):\n",
    "    aff_1 = aff_list_1[i]\n",
    "    aff_2 = aff_list_2[i]\n",
    "    if aff_1 not in aff_coordinate_dict or aff_2 not in aff_coordinate_dict: continue\n",
    "    lat1, lon1 = aff_coordinate_dict[aff_1]\n",
    "    lat2, lon2 = aff_coordinate_dict[aff_2]\n",
    "\n",
    "    aff_pair = str(aff_1)+'-'+str(aff_2)         # str(smaller affID)+'-'+str(larger affID)\n",
    "    distance = haversine(lat1, lon1, lat2, lon2) # km\n",
    "    Aff_pair_distance[aff_pair] = distance\n",
    "del aff_list_1,aff_list_2,aff_coordinate_dict\n",
    "gc.collect()\n",
    "\n",
    "pickle.dump(Aff_pair_distance, open(\"%s/data/processed/AffID_pair_Distance.pickle\"%(pre_path), 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636a2ea",
   "metadata": {},
   "source": [
    "2) We then calculate the average distance among a paper's affiliation list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the affliation pair distance dictionary.\n",
    "# {affID_1-affID_2: distance}\n",
    "Aff_pair_distance = pd.read_pickle(open(\"%s/data/processed/AffID_pair_Distance.pickle\"%(pre_path), 'rb'))\n",
    "\n",
    "\n",
    "# Load the SciSciNet_PaperAuthorAffiliations.tsv file to extract the affiliation of papers.\n",
    "# column: [PaperID, AuthorID, AffiliationID, AuthorSequenceNumber]\n",
    "Paper_Affs_df = read_big_csv(\"%s/data/raw/SciSciNet_PaperAuthorAffiliations.tsv\"%pre_path, sep='\\t', compression=None, chunksize=1000000, nrows=None, \n",
    "                           usecols=['PaperID', 'AffiliationID'])\n",
    "\n",
    "# count the number of affiliations for each paper\n",
    "Paper_Affs_df['PaperCount'] = Paper_Affs_df.groupby('PaperID')['PaperID'].transform('count')\n",
    "# select papers with more than one affiliation\n",
    "paper_multi_aff_df = Paper_Affs_df[Paper_Affs_df['PaperCount']>1]\n",
    "paper_multi_aff_df = paper_multi_aff_df.drop(columns=['PaperCount'])\n",
    "\n",
    "# for multiple affiliation PaperID, select rows where 'AffiliationID' is NaN (in case 'NAN' becomes a ghost AffiliationID)\n",
    "nan_affiliation_paper_ids = set(paper_multi_aff_df[paper_multi_aff_df['AffiliationID'].isna()]['PaperID'].tolist())\n",
    "\n",
    "# remove multiple affiliation PaperID with NaN AffiliationID from Paper_Affs_df\n",
    "Paper_Affs_df = Paper_Affs_df[~Paper_Affs_df['PaperID'].isin(nan_affiliation_paper_ids)]\n",
    "Paper_Affs_df['AffiliationID'] = Paper_Affs_df['AffiliationID'].fillna(-1) # for single affiliation papers, fill NaN AffiliationID with -1 if it exists\n",
    "Paper_Affs_df['AffiliationID'] = Paper_Affs_df['AffiliationID'].astype(int)\n",
    "Paper_Affs_df = Paper_Affs_df.sort_values(by=['PaperID','AffiliationID'], ascending=[True,True]) # sorted by PaperID and AffiliationID\n",
    "Paper_Affs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "###################################\n",
    "paper_list = list(Paper_Affs_df['PaperID'].drop_duplicates()) # get the unique paper IDs\n",
    "paper_aff_counter = Counter(Paper_Affs_df['PaperID'].tolist()) # Count the number of affiliations for each paper.\n",
    "\n",
    "team_distance = defaultdict(float) # team_distance is a dictionary that stores the distance between each ever collaborated affiliationID pair for each paper.\n",
    "end_tag = 0\n",
    "for paper in paper_list:\n",
    "    start_tag = end_tag\n",
    "    end_tag = start_tag + paper_aff_counter[paper]\n",
    "    df_paper = Paper_Affs_df[start_tag:end_tag]\n",
    "    if not isinstance(df_paper, pd.DataFrame): continue # Check if df_paper is a DataFrame.\n",
    "    aff_list = list(df_paper['AffiliationID'].drop_duplicates()) # get the sorted unique affiliation IDs for the paper.\n",
    "    \n",
    "    distinc_aff_count = len(aff_list)\n",
    "    if distinc_aff_count == 1: # assign 0 distance for single affiliation papers\n",
    "        team_distance[paper] = 0\n",
    "\n",
    "    else:\n",
    "        aff_pair_counts = defaultdict(int) # Initialize a dictionary to count the frequency of each affiliation pair.\n",
    "        for i in range(distinc_aff_count): # smaller affID\n",
    "            aff_i = aff_list[i]\n",
    "            for j in range(i,distinc_aff_count): # larger affID\n",
    "                aff_j = aff_list[j]\n",
    "                aff_pair = str(aff_i)+'-'+str(aff_j)         # str(smaller affID)+'-'+str(largeer affID)\n",
    "                aff_pair_counts[aff_pair] += 1\n",
    "        \n",
    "        aff_pair_count_sum, aff_pair_distance_sum = 0, 0 # Initialize the sum of counts and distances for affiliation pairs.\n",
    "        for aff_pair in aff_pair_counts: \n",
    "            if aff_pair in Aff_pair_distance:\n",
    "                aff_pair_frequence = aff_pair_counts[aff_pair]\n",
    "                aff_pair_count_sum += aff_pair_frequence\n",
    "                \n",
    "                aff_pair_distance = Aff_pair_distance[aff_pair]\n",
    "                aff_pair_distance_sum += aff_pair_distance*aff_pair_frequence\n",
    "        # Calculate the average distance for the paper based on the frequency of each affiliation pair.\n",
    "        if aff_pair_count_sum == 0: # Skip papers with no affiliation pairs.\n",
    "            continue\n",
    "        else:\n",
    "            team_distance[paper] = aff_pair_distance_sum/aff_pair_count_sum\n",
    "\n",
    "del Aff_pair_distance, Paper_Affs_df, paper_list, paper_aff_counter\n",
    "gc.collect()\n",
    "\n",
    "pickle.dump(team_distance, open(\"%s/data/processed/PaperID_Team_Distance.pickle\"%pre_path, 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c67605a",
   "metadata": {},
   "source": [
    "## Merge all metrics into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8063568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import pandas as pd\n",
    "###############################################\n",
    "\n",
    "# Load PaperID_Year and select papers published between 1950 and 2021.\n",
    "paper_year = pickle.load(open('%s/data/processed/PaperID_Year.pickle'%pre_path, 'rb'))\n",
    "df_year = pd.DataFrame(list(paper_year.items()), columns=['PaperID', 'Year'])\n",
    "df_year = df_year[(df_year['Year'] >= 1950) & (df_year['Year'] <= 2021)]\n",
    "print('paper_year_selected, count:',len(df_year))\n",
    "\n",
    "# Load PaperID_DocType and select papers with 'Journal' or 'Conference' document types.\n",
    "paper_DocType = pickle.load(open(\"%s/data/processed/PaperID_DocType.pickle\"%pre_path, \"rb\"))\n",
    "df_doct_type = pd.DataFrame(list(paper_DocType.items()), columns=['PaperID', 'DocType'])\n",
    "df_doct_type = df_doct_type[(df_doct_type['DocType'] == 'Journal') | (df_doct_type['DocType'] == 'Conference')] # ['Journal','Thesis','Conference','Book','BookChapter','Repository','Dataset']\n",
    "print('paper_DocType_selected, count:',len(df_doct_type))\n",
    "\n",
    "# Select papers that are both in the selected year range and have the selected document types.\n",
    "df_doct_type_select = pd.merge(df_year, df_doct_type, how='inner', on = 'PaperID')\n",
    "df_merge = df_doct_type_select.drop(columns=['DocType'])\n",
    "del paper_year,df_year,paper_DocType,df_doct_type\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# merge the paper properties with the pub year and doc type, left join.\n",
    "for property_ in ['TopField','ISSN_SJR','SB_B', 'SB_T','C5','Copen','Atyp_10pct_Z','Team_Size','Team_Distance','Team_Freshness','reference_Count','reference_Age','reference_C5']:\n",
    "    paper_property = pickle.load(open('%s/data/processed/PaperID_%s.pickle'%(pre_path,property_), 'rb'))\n",
    "    paper_property_df = pd.DataFrame(list(paper_property.items()), columns=['PaperID', property_])\n",
    "    print('paper_%s, count:%s'%(property_,len(paper_property_df)))\n",
    "    df_merge = df_merge.merge(paper_property_df, how='left', on = 'PaperID')\n",
    "df_merge.rename(columns={'TopField':'Field', 'ISSN_SJR':'SJR'}, inplace=True)\n",
    "df_merge['Novelty_90pct'] = df_merge['Atyp_10pct_Z']*(-1) # convert Atyp_10pct_Z to Novelty_90pct, which is the negative of Atyp_10pct_Z.\n",
    "df_merge = df_merge.drop(columns=['Atyp_10pct_Z'])\n",
    "print('\\n\\n\\n\\n')\n",
    "\n",
    "# merge KI and DC properties with the pub year, doc type and other paper properties.\n",
    "for KI_type in ['KI2', 'KI2_frac', 'KI2_adj', 'KI2_adj_frac'][:1]:\n",
    "    paper_DR = pickle.load(open(\"%s/data/processed/PaperID_%s.pickle\"%(pre_path,KI_type), \"rb\"))\n",
    "    paper_property_df = pd.DataFrame({'PaperID':list(paper_DR.keys()), KI_type:list(paper_DR.values())})\n",
    "    print('paper_%s, count:%s'%(KI_type,len(paper_property_df)))\n",
    "    df_DR = df_merge.merge(paper_property_df, how='left', on = 'PaperID')\n",
    "    del paper_DR\n",
    "    gc.collect()\n",
    "    \n",
    "    paper_reference_DR = pickle.load(open(\"%s/data/processed/PaperID_reference_%s.pickle\"%(pre_path,KI_type), \"rb\"))\n",
    "    paper_property_df = pd.DataFrame({'PaperID':list(paper_reference_DR.keys()), 'reference_'+KI_type:list(paper_reference_DR.values())})\n",
    "    print('paper_reference_%s, count:%s'%(KI_type,len(paper_property_df)))\n",
    "    df_DR = df_merge.merge(paper_property_df, how='left', on = 'PaperID')\n",
    "    del paper_reference_DR\n",
    "    gc.collect()\n",
    "    \n",
    "    for DC_type in ['Dopen_nok','Dopen','D5_nok','D5'][:1]:\n",
    "\n",
    "        paper_DC = pickle.load(open(\"%s/data/processed/PaperID_%s.pickle\"%(pre_path,DC_type), \"rb\"))\n",
    "        paper_property_df = pd.DataFrame({'PaperID':list(paper_DC.keys()), DC_type:list(paper_DC.values())})\n",
    "        print('paper_%s, count:%s'%(DC_type,len(paper_property_df)))\n",
    "        df_KI_DC = df_DR.merge(paper_property_df, how='left', on = 'PaperID')\n",
    "        del paper_DC\n",
    "        gc.collect()\n",
    "        \n",
    "        paper_reference_DC = pickle.load(open(\"%s/data/processed/PaperID_reference_%s.pickle\"%(pre_path,DC_type), \"rb\"))\n",
    "        paper_property_df = pd.DataFrame({'PaperID':list(paper_reference_DC.keys()), 'reference_'+DC_type:list(paper_reference_DC.values())})\n",
    "        print('paper_reference_%s, count:%s'%(DC_type,len(paper_property_df)))\n",
    "        df_KI_DC = df_KI_DC.merge(paper_property_df, how='left', on = 'PaperID')\n",
    "        del paper_reference_DC\n",
    "        gc.collect()\n",
    "     \n",
    "        print(df_KI_DC)\n",
    "        print('\\n\\n\\n\\n')\n",
    "        df_KI_DC.to_pickle('%s/data/processed/PaperID_%s-%s_merged.pickle'%(pre_path,KI_type,DC_type))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
